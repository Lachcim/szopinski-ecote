\documentclass{article}

\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{hyperref}
\setlength{\parskip}{1em}

\begin{document}

	\title{ECOTE preliminary report:\\
	Top-down parser with backtracking}
	\author{Michał Szopiński 300182}
	\date{May 11, 2022}
	\maketitle

	\section{General overview and assumptions}

	The goal of this project is to write a program to parse and produce a syntax
	tree for an arbitrary input file using an arbitrary grammar.

	The parsing is to be implemented using a top-down recursive descent
	algorithm, i.e. one that attempts to find a combination of productions
	matching the input token sequence, starting from the root production.
	Backtracking means that the algorithm may abandon previously chosen
	productions if it discovers that they cannot lead to a match.

	Because a parser operates on tokens, which are produced during the lexical
	analysis stage, the program must have a built-in lexer utility. To reduce
	complexity, the lexeme recognition algorithm is hard-coded and not
	customizable. The built-in lexer recognizes tokens that are common to
	popular C-like languages.

	As mentioned before, the program checks arbitrary inputs against arbitrary
	grammars. This implies that the user supplies two files, one containing
	the input and one containing a description of the grammar.

	The program tokenizes both files using the built-in lexer and parses the
	grammar description file using a hard-coded grammar description
	meta-language. The produced syntax tree is then validated and transformed
	into a grammar descriptor object, which is in turn used to parse the input
	file. As such, the same parser may be used to process both input files.

	The program implements rudimentary diagnostics and error handling. In
	particular, the user may receive lexical, parse and semantic errors during
	each stage of processing. Changes in the syntax tree are also displayed
	as they occur.

	\section{Functional requirements}

	The programming language of choice for this project is Python. Its dynamic
	typing makes it suitable for straightforward operations on complex data
	types. The previous proposal of using C/C++ has been withdrawn.

	\subsection{Lexical analysis}

	Because the lexical analyser is hard-coded, it must strive to resemble the
	lexical ruleset of mainstream C-like languages, so as to match user
	expectations. A set of popular token categories is defined:

	\begin{center}
	\begin{tabular}{ |c|p{2.5cm}|p{6cm}| }
		\hline
			Category & Examples & Description \\
		\hline
			Identifier & \texttt{hello\_world123} & Used for variable names and keywords. \\
		\hline
			Operator & \texttt{\$ ++ ===} & Used to define multiple-character non-identifier entities. \\
		\hline
			Separator & \texttt{, ; ( \}} & Used to define single-character non-identifier entities, typically neighboring each other. \\
		\hline
			String literal & \texttt{"can't" 'won\textbackslash't'} & Incorporates rules for string enclosure and escaping. \\
		\hline
			Number literal & \texttt{123 +1.0} & Incorporates rules for digit sequences, sign prefixes and decimal points. \\
		\hline
			Comment & \texttt{//hello \newline /* world */} & Incorporates rules for single-line and multi-line comments. \\
		\hline
			Invalid & \texttt{123abc "hello} & Marks lexical errors. Used for diagnostics. \\
		\hline
			End of file & & Denotes the end of the input file. Used for grammar description. \\
		\hline
	\end{tabular}
	\end{center}

	\subsubsection{Scanning and evaluation}

	Most of the above tokens are produced during the scanning phase. The
	end-of-file token is appended at the end of the token sequence during the
	evaluation phase. Comment tokens are removed from the sequence before they
	reach the parser. The presence of invalid tokens prevents the program from
	progressing to the parsing phase.

	\subsection{Grammar description meta-language}

	Once the grammar description file has been tokenized using the universal
	lexer, the program applies a predefined meta-grammar to parse the file into
	a syntax tree for further processing.

	At the top level, the meta-language is a set of definitions describing each
	production in the language. The fundamental building blocks for definitions
	are binary \textbf{compound expressions} and \textbf{terminal expressions}.

	Compound expressions are the framework for backtracking recursive descent
	logic. They accept two arguments and define the logical relation between
	them. Three such expressions are defined:

	\begin{enumerate}
		\item \textbf{Concatenation} - accepts if both arguments accept.
		\item \textbf{Optional concatenation} - accepts if either both or only
		the second argument accepts.
		\item \textbf{Alternative} - accepts if either argument accepts.
	\end{enumerate}

	Terminal expressions are used to describe the terminal symbols of the
	language. Three kinds of such tokens may be discerned:

	\begin{enumerate}
		\item \textbf{String literal} - accepts a token of any category whose
		value is equal to that enclosed in the literal.
		\item \textbf{Identifier}
		\begin{enumerate}
			\item \textbf{Reserved identifier} - identifier belonging to the set \texttt{identifier string\_literal number\_literal end\_of\_file}.
			Accepts a token of any value belonging to the matching category.
			\item \textbf{Arbitrary identifier} - resolves to a different definition in the grammar.
		\end{enumerate}
	\end{enumerate}

	\subsubsection{Formal definition of the meta-language}

	The following is a formal definition of the above rules, written as a
	grammar description object using Python syntax:

	\scriptsize\begin{verbatim}meta_grammar = {
    "root": Alternative(
        "definitions",
        Terminal("end_of_file")
    ),
    "definitions": Concatenation(
        "definition",
        Alternative(
            "definitions",
            Terminal("end_of_file")
        )
    ),
    "definition": Concatenation(
        "definition_key",
        Concatenation(
            Terminal("operator", "="),
            Concatenation(
                "definition_expression",
                Terminal("separator", ";")
            )
        )
    ),
    "definition_key": Terminal("identifier"),
    "definition_expression": "expression",
    "expression": Alternative(
        "concat_expression",
        Alternative(
            "opt_concat_expression",
            Alternative(
                "alt_expression",
                Alternative(
                    "expr_identifier",
                    "expr_string_literal"
                )
            )
        )
    ),
    "expr_identifier": Terminal("identifier"),
    "expr_string_literal": Terminal("string_literal"),
    "concat_expression": Concatenation(
        Terminal("identifier", "concat"),
        "argument"
    ),
    "opt_concat_expression": Concatenation(
        Terminal("identifier", "opt_concat"),
        "argument"
    ),
    "alt_expression": Concatenation(
        Terminal("identifier", "alt"),
        "argument"
    ),
    "argument": Concatenation(
        Terminal("separator", "("),
        Concatenation(
            "expr_arg1",
            Concatenation(
                Terminal("separator", ","),
                Concatenation(
                    "expr_arg2",
                    Terminal("separator", ")")
                )
            )
        )
    ),
    "expr_arg1": "expression",
    "expr_arg2": "expression"
}\end{verbatim}

\end{document}
