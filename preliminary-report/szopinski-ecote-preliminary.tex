\documentclass{article}

\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{hyperref}
\setlength{\parskip}{1em}

\begin{document}

	\title{ECOTE preliminary report:\\
	Top-down parser with backtracking}
	\author{Michał Szopiński 300182}
	\date{May 11, 2022}
	\maketitle

	\section{General overview and assumptions}

	The goal of this project is to write a program to parse and produce a syntax
	tree for an arbitrary input file using an arbitrary grammar.

	The parsing is to be implemented using a top-down recursive descent
	algorithm, i.e. one that attempts to find a combination of productions
	matching the input token sequence, starting from the root production.
	Backtracking means that the algorithm may abandon previously chosen
	productions if it discovers that they cannot lead to a match.

	Because a parser operates on tokens, which are produced during the lexical
	analysis stage, the program must have a built-in lexer utility. To reduce
	complexity, the lexeme recognition algorithm is hard-coded and not
	customizable. The built-in lexer recognizes tokens that are common to
	popular C-like languages.

	As mentioned before, the program checks arbitrary inputs against arbitrary
	grammars. This implies that the user supplies two files, one containing
	the input and one containing a description of the grammar.

	The program tokenizes both files using the built-in lexer and parses the
	grammar description file using a hard-coded grammar description
	meta-language. The produced syntax tree is then validated and transformed
	into a grammar descriptor object, which is in turn used to parse the input
	file. As such, the same parser may be used to process both input files.

	The program implements rudimentary diagnostics and error handling. In
	particular, the user may receive lexical, parse and semantic errors during
	each stage of processing. Changes in the syntax tree are also displayed
	as they occur.

	\section{Functional requirements}

	The programming language of choice for this project is Python. Its dynamic
	typing makes it suitable for straightforward operations on complex data
	types. The previous proposal of using C/C++ has been withdrawn.

	\subsection{Lexical analysis}

	Because the lexical analyser is hard-coded, it must strive to resemble the
	lexical ruleset of mainstream C-like languages, so as to match user
	expectations. A set of popular token categories is defined:

	\begin{center}
	\begin{tabular}{ |c|p{2.5cm}|p{6cm}| }
		\hline
			Category & Examples & Description \\
		\hline
			Identifier & \texttt{hello\_world123} & Used for variable names and keywords. \\
		\hline
			Operator & \texttt{\$ ++ ===} & Used to define multiple-character non-identifier entities. \\
		\hline
			Separator & \texttt{, ; ( \}} & Used to define single-character non-identifier entities, typically neighboring each other. \\
		\hline
			String literal & \texttt{"can't" 'won\textbackslash't'} & Incorporates rules for string enclosure and escaping. \\
		\hline
			Number literal & \texttt{123 +1.0} & Incorporates rules for digit sequences, sign prefixes and decimal points. \\
		\hline
			Comment & \texttt{//hello \newline /* world */} & Incorporates rules for single-line and multi-line comments. \\
		\hline
			Invalid & \texttt{123abc "hello} & Marks lexical errors. Used for diagnostics. \\
		\hline
			End of file & & Denotes the end of the input file. Used for grammar description. \\
		\hline
	\end{tabular}
	\end{center}

	\subsubsection{Scanning and evaluation}

	Most of the above tokens are produced during the scanning phase. The
	end-of-file token is appended at the end of the token sequence during the
	evaluation phase. Comment tokens are removed from the sequence before they
	reach the parser. The presence of invalid tokens prevents the program from
	progressing to the parsing phase.

	\subsection{Grammar description meta-language}

	Once the grammar description file has been tokenized using the universal
	lexer, the program applies a predefined meta-grammar to parse the file into
	a syntax tree for further processing.

	At the top level, the meta-language is a set of definitions describing each
	production in the language. The fundamental building blocks for definitions
	are binary \textbf{compound expressions} and \textbf{terminal expressions}.

	Compound expressions are the framework for backtracking recursive descent
	logic. They accept two arguments and define the logical relation between
	them. Three such expressions are defined:

	\begin{enumerate}
		\item \textbf{Concatenation} - accepts if both arguments accept.
		\item \textbf{Optional concatenation} - accepts if either both or only
		the second argument accepts.
		\item \textbf{Alternative} - accepts if either argument accepts.
	\end{enumerate}

	Terminal expressions are used to describe the terminal symbols of the
	language. Three kinds of such tokens may be discerned:

	\begin{enumerate}
		\item \textbf{String literal} - accepts a token of any category whose
		value is equal to that enclosed in the literal.
		\item \textbf{Identifier}
		\begin{enumerate}
			\item \textbf{Reserved identifier} - identifier belonging to the set \texttt{identifier string\_literal number\_literal end\_of\_file}.
			Accepts a token of any value belonging to the matching category.
			\item \textbf{Arbitrary identifier} - resolves to a different definition in the grammar.
		\end{enumerate}
	\end{enumerate}

	\subsubsection{Formal description of the meta-language}

	The following is a formal description of the above rules, written as a
	grammar description object using Python syntax:

	\scriptsize\begin{verbatim}meta_grammar = {
    "root": Alternative(
        "definitions",
        Terminal("end_of_file")
    ),
    "definitions": Concatenation(
        "definition",
        Alternative(
            "definitions",
            Terminal("end_of_file")
        )
    ),
    "definition": Concatenation(
        "definition_key",
        Concatenation(
            Terminal("operator", "="),
            Concatenation(
                "definition_expression",
                Terminal("separator", ";")
            )
        )
    ),
    "definition_key": Terminal("identifier"),
    "definition_expression": "expression",
    "expression": Alternative(
        "concat_expression",
        Alternative(
            "opt_concat_expression",
            Alternative(
                "alt_expression",
                Alternative(
                    "expr_identifier",
                    "expr_string_literal"
                )
            )
        )
    ),
    "expr_identifier": Terminal("identifier"),
    "expr_string_literal": Terminal("string_literal"),
    "concat_expression": Concatenation(
        Terminal("identifier", "concat"),
        "argument"
    ),
    "opt_concat_expression": Concatenation(
        Terminal("identifier", "opt_concat"),
        "argument"
    ),
    "alt_expression": Concatenation(
        Terminal("identifier", "alt"),
        "argument"
    ),
    "argument": Concatenation(
        Terminal("separator", "("),
        Concatenation(
            "expr_arg1",
            Concatenation(
                Terminal("separator", ","),
                Concatenation(
                    "expr_arg2",
                    Terminal("separator", ")")
                )
            )
        )
    ),
    "expr_arg1": "expression",
    "expr_arg2": "expression"
}\end{verbatim}

	\normalsize There are two additional semantic constraints: (1) there must
	be a definition named \texttt{root}, and (2) there mustn't be any
	definitions whose names belong to the set of reserved identifiers.

	\subsection{Top-down parser}

	The parser is the core feature of the software. It takes the root production
	of the given grammar and attempts to find a set of productions stemming from
	the root which could accept all the tokens in the sequence. It does so by
	implementing the logical rules of the three compound expressions discussed
	earlier.

	Each step of the parser is a recursive call to a function which processes
	a single binary or terminal production. If it is determined that the set of
	logical rules for that production can not yield a combination of productions to
	parse the entire token sequence, the function generates an exception and returns
	control to its caller.

	Exceptions don't originate at compound productions, they are merely propagated
	upwards by them. All exceptions stem from terminal productions at the leaves
	of the production tree. A terminal symbol matches the current token in the
	sequence against its signature and either increments the token iterator
	(''accepts" the token), or raises an error to be handled by the logic of
	compound productions higher in the syntax tree.

	Backtracking is achieved by remembering the state of the token iterator at
	the initialization of a compound production. If one path fails to parse
	the token sequence, the iterator is reset and a different path is tried.
	If neither path succeeds, the error from the later path is propagated
	upwards, where backtracking may occur as well. If both paths are exhausted
	at the root level, the token tree is declared unparseable.

	The above algorithm merely checks the validity of the token sequence against
	the grammar. To build a parse tree, each call to the parsing function may
	additionally result in the addition of a node to a data structure mirroring
	the history of chosen productions. Backtracking rules apply.

\end{document}
